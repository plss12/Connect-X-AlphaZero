{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aff7727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO: Successfully loaded OpenSpiel environments: 8.\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_chess\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_connect_four\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_gin_rummy\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_go\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_tic_tac_toe\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_universal_poker\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_repeated_poker\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO:    open_spiel_python_repeated_pokerkit\n",
      "[kaggle_environments.envs.open_spiel_env.open_spiel_env] INFO: OpenSpiel games skipped: 0.\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from kaggle_environments import evaluate, make, utils\n",
    "import os\n",
    "import sys\n",
    "import inspect\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import gymnasium as gym\n",
    "from tianshou.algorithm.modelfree.dqn import DiscreteQLearningPolicy\n",
    "\n",
    "from Submissions.mcts_agent import mcts_agent\n",
    "from Submissions.minimax_agent import minimax_agent\n",
    "from Submissions.rainbow.rainbow_agent import rainbow_agent\n",
    "from Submissions.ppo.ppo_agent import ppo_agent\n",
    "\n",
    "env = make(\"connectx\", debug=True)\n",
    "print(env.render(mode=\"ansi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37873b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random agent\n",
    "def random_agent(observation, configuration):\n",
    "    from random import choice\n",
    "    return choice([c for c in range(configuration.columns) if observation.board[c] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad0d6e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluating agent against random and negamax\n",
    "def mean_reward(rewards, role):\n",
    "    if role == \"P1\":\n",
    "        return np.mean([r[0] for r in rewards])\n",
    "    else:\n",
    "        return np.mean([r[1] for r in rewards])\n",
    "\n",
    "def evaluate_agent(agent, num_episodes=10):\n",
    "    rewards_random_p1 = evaluate(\"connectx\", [agent, \"random\"], num_episodes=num_episodes)\n",
    "    rewards_random_p2 = evaluate(\"connectx\", [\"random\", agent], num_episodes=num_episodes)\n",
    "    rewards_negamax_p1 = evaluate(\"connectx\", [agent, \"negamax\"], num_episodes=num_episodes)\n",
    "    rewards_negamax_p2 = evaluate(\"connectx\", [\"negamax\", agent], num_episodes=num_episodes)\n",
    "\n",
    "    print(f\"Evaluating agent against random and negamax with {num_episodes} games as P1 and P2...\\n\")\n",
    "    \n",
    "    print(f\"P1 vs Random: {mean_reward(rewards_random_p1, 'P1')}\")\n",
    "    print(f\"P1 vs Negamax: {mean_reward(rewards_negamax_p1, 'P1')}\\n\")  \n",
    "\n",
    "    print(f\"P2 vs Random: {mean_reward(rewards_random_p2, 'P2')}\")\n",
    "    print(f\"P2 vs Negamax: {mean_reward(rewards_negamax_p2, 'P2')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "469387f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging agent against random\n",
    "def debug_agent(agent):\n",
    "    env = make(\"connectx\", debug=True)\n",
    "    trainer = env.train([None, \"random\"])\n",
    "    observation = trainer.reset()\n",
    "\n",
    "    while not env.done:\n",
    "        t0 = time.time()\n",
    "        my_action = agent(observation, env.configuration)\n",
    "        t1 = time.time()\n",
    "        print(f\"Turn {observation.step + 1}: Action {my_action} (in {t1 - t0:.2f} seconds)\")\n",
    "        observation, reward, done, info = trainer.step(my_action)   \n",
    "\n",
    "    print(f\"\\nYou won :)\\n\" if env.state[0].reward > 0 else \"\\nYou lost :(\\n\")\n",
    "    print(env.render(mode=\"ansi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360fd1c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Facing agents\n",
    "def facing_agents(agent_a, agent_b, num_episodes=10):\n",
    "    a_rewards = []\n",
    "    b_rewards = []\n",
    "\n",
    "    env = make(\"connectx\", debug=True)\n",
    "\n",
    "    for i in range(num_episodes):\n",
    "        if i < num_episodes // 2:\n",
    "            env.run([agent_a, agent_b])\n",
    "            a_rewards.append(env.state[0].reward)\n",
    "            b_rewards.append(env.state[1].reward)\n",
    "        else:\n",
    "            env.run([agent_b, agent_a])\n",
    "            b_rewards.append(env.state[0].reward)\n",
    "            a_rewards.append(env.state[1].reward)\n",
    "\n",
    "    print(f\"Agent A won {a_rewards.count(1)} times\")\n",
    "    print(f\"Agent B won {b_rewards.count(1)} times\")\n",
    "    print(f\"Draw {a_rewards.count(0)} times\")\n",
    "    return a_rewards, b_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3104416d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Action 2 (in 0.36 seconds)\n",
      "Turn 3: Action 3 (in 0.45 seconds)\n",
      "Turn 5: Action 4 (in 0.30 seconds)\n",
      "Turn 7: Action 3 (in 0.47 seconds)\n",
      "Turn 9: Action 3 (in 0.61 seconds)\n",
      "Turn 11: Action 3 (in 0.29 seconds)\n",
      "\n",
      "You won :)\n",
      "\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 2 | 1 | 2 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 2 | 0 | 1 | 2 | 1 | 0 | 2 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n",
      "Evaluating agent against random and negamax with 10 games as P1 and P2...\n",
      "\n",
      "P1 vs Random: 1.0\n",
      "P1 vs Negamax: 1.0\n",
      "\n",
      "P2 vs Random: 1.0\n",
      "P2 vs Negamax: 0.9\n"
     ]
    }
   ],
   "source": [
    "# Debugging and evaluating Minimax\n",
    "debug_agent(minimax_agent)\n",
    "evaluate_agent(minimax_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db86486f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1: Action 3 (in 1.90 seconds)\n",
      "Turn 3: Action 3 (in 1.90 seconds)\n",
      "Turn 5: Action 3 (in 1.90 seconds)\n",
      "Turn 7: Action 3 (in 1.90 seconds)\n",
      "\n",
      "You won :)\n",
      "\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 0 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 2 | 0 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "| 0 | 2 | 2 | 1 | 0 | 0 | 0 |\n",
      "+---+---+---+---+---+---+---+\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Debugging and evaluating MCTS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m debug_agent(mcts_agent)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mevaluate_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmcts_agent\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 9\u001b[0m, in \u001b[0;36mevaluate_agent\u001b[0;34m(agent, num_episodes)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mevaluate_agent\u001b[39m(agent, num_episodes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 9\u001b[0m     rewards_random_p1 \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconnectx\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrandom\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     rewards_random_p2 \u001b[38;5;241m=\u001b[39m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectx\u001b[39m\u001b[38;5;124m\"\u001b[39m, [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom\u001b[39m\u001b[38;5;124m\"\u001b[39m, agent], num_episodes\u001b[38;5;241m=\u001b[39mnum_episodes)\n\u001b[1;32m     11\u001b[0m     rewards_negamax_p1 \u001b[38;5;241m=\u001b[39m evaluate(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconnectx\u001b[39m\u001b[38;5;124m\"\u001b[39m, [agent, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegamax\u001b[39m\u001b[38;5;124m\"\u001b[39m], num_episodes\u001b[38;5;241m=\u001b[39mnum_episodes)\n",
      "File \u001b[0;32m~/miniconda3/envs/ConnectX/lib/python3.11/site-packages/kaggle_environments/core.py:74\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(environment, agents, configuration, steps, num_episodes, debug, state)\u001b[0m\n\u001b[1;32m     72\u001b[0m rewards \u001b[38;5;241m=\u001b[39m [[] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes)]\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_episodes):\n\u001b[0;32m---> 74\u001b[0m     last_state \u001b[38;5;241m=\u001b[39m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43magents\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     75\u001b[0m     rewards[i] \u001b[38;5;241m=\u001b[39m [state\u001b[38;5;241m.\u001b[39mreward \u001b[38;5;28;01mfor\u001b[39;00m state \u001b[38;5;129;01min\u001b[39;00m last_state]\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m rewards\n",
      "File \u001b[0;32m~/miniconda3/envs/ConnectX/lib/python3.11/site-packages/kaggle_environments/core.py:284\u001b[0m, in \u001b[0;36mEnvironment.run\u001b[0;34m(self, agents)\u001b[0m\n\u001b[1;32m    282\u001b[0m start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m perf_counter() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mrunTimeout:\n\u001b[0;32m--> 284\u001b[0m     actions, logs \u001b[38;5;241m=\u001b[39m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep(actions, logs)\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;129;01mand\u001b[39;00m perf_counter() \u001b[38;5;241m-\u001b[39m start \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfiguration\u001b[38;5;241m.\u001b[39mrunTimeout:\n",
      "File \u001b[0;32m~/miniconda3/envs/ConnectX/lib/python3.11/site-packages/kaggle_environments/core.py:700\u001b[0m, in \u001b[0;36mEnvironment.__agent_runner.<locals>.act\u001b[0;34m(none_action)\u001b[0m\n\u001b[1;32m    698\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool\u001b[38;5;241m.\u001b[39mmap(act_agent, act_args)\n\u001b[1;32m    699\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 700\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mmap\u001b[39m(act_agent, act_args))\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# results is a list of tuples where the first element is an agent action and the second is the agent log\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# This destructures into two lists, a list of actions and a list of logs.\u001b[39;00m\n\u001b[1;32m    704\u001b[0m actions, logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mresults)\n",
      "File \u001b[0;32m~/miniconda3/envs/ConnectX/lib/python3.11/site-packages/kaggle_environments/core.py:137\u001b[0m, in \u001b[0;36mact_agent\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m none_action, {}\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mobservation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ConnectX/lib/python3.11/site-packages/kaggle_environments/agent.py:183\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    182\u001b[0m     start \u001b[38;5;241m=\u001b[39m perf_counter()\n\u001b[0;32m--> 183\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    185\u001b[0m     traceback\u001b[38;5;241m.\u001b[39mprint_exc(file\u001b[38;5;241m=\u001b[39merr_buffer)\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:255\u001b[0m, in \u001b[0;36mmcts_agent\u001b[0;34m(observation, configuration)\u001b[0m\n\u001b[1;32m    245\u001b[0m pyplai_state \u001b[38;5;241m=\u001b[39m ConnectXState(current_board, current_player)\n\u001b[1;32m    247\u001b[0m mcts_solver \u001b[38;5;241m=\u001b[39m MCTS(\n\u001b[1;32m    248\u001b[0m      ConnectXState\u001b[38;5;241m.\u001b[39mapply_move,\n\u001b[1;32m    249\u001b[0m      ConnectXState\u001b[38;5;241m.\u001b[39mget_moves, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    252\u001b[0m      \u001b[38;5;241m2\u001b[39m, \n\u001b[1;32m    253\u001b[0m      limit_time)\n\u001b[0;32m--> 255\u001b[0m recommended_move \u001b[38;5;241m=\u001b[39m \u001b[43mmcts_solver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mejecuta\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpyplai_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m recommended_move\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:61\u001b[0m, in \u001b[0;36mMCTS.ejecuta\u001b[0;34m(self, s0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     s1\u001b[38;5;241m=\u001b[39mtree[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     60\u001b[0m     movs\u001b[38;5;241m=\u001b[39mtree[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m---> 61\u001b[0m     delta \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefault_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mmovs\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjugadores\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbackup(v1,delta,\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjugadores)\n\u001b[1;32m     63\u001b[0m jugador \u001b[38;5;241m=\u001b[39m s0\u001b[38;5;241m.\u001b[39mjugadorActual\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:113\u001b[0m, in \u001b[0;36mMCTS.default_policy\u001b[0;34m(self, s, movs, jugadores)\u001b[0m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdefault_policy\u001b[39m(\u001b[38;5;28mself\u001b[39m, s,movs,jugadores):\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;66;03m#El jugador que queremos comprobar es el anterior al del estado actual, ya que se ha cambiado en tree_policy\u001b[39;00m\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mes_estado_final\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    114\u001b[0m         a \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39mchoice(movs)\n\u001b[1;32m    115\u001b[0m         s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maplicar_movimiento(s,a)\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:183\u001b[0m, in \u001b[0;36mConnectXState.is_final_state\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_movs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwins_player(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwins_player\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:192\u001b[0m, in \u001b[0;36mConnectXState.wins_player\u001b[0;34m(self, player)\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mROWS):\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOLUMNS):\n\u001b[0;32m--> 192\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_piece\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m player:\n\u001b[1;32m    193\u001b[0m             directions \u001b[38;5;241m=\u001b[39m [(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)]\n\u001b[1;32m    195\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m dr, dc \u001b[38;5;129;01min\u001b[39;00m directions:\n",
      "File \u001b[0;32m~/Connect-X-Agents/Submissions/mcts_agent.py:151\u001b[0m, in \u001b[0;36mConnectXState.get_piece\u001b[0;34m(self, row, col)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(board_data)\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjugadorActual \u001b[38;5;241m=\u001b[39m current_player\n\u001b[0;32m--> 151\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_piece\u001b[39m(\u001b[38;5;28mself\u001b[39m, row, col):\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mboard[row \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mCOLUMNS \u001b[38;5;241m+\u001b[39m col]\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mget_moves\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Debugging and evaluating MCTS\n",
    "debug_agent(mcts_agent)\n",
    "evaluate_agent(mcts_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b8740c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging and evaluating rainbow\n",
    "debug_agent(rainbow_agent)\n",
    "evaluate_agent(rainbow_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e55a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Debugging and evaluating ppo\n",
    "debug_agent(ppo_agent)\n",
    "evaluate_agent(ppo_agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3e993a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Facing Minimax vs Rainbow\n",
      "Agent A won 10 times\n",
      "Agent B won 0 times\n",
      "Draw 0 times\n",
      "([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n",
      "\n",
      "Facing Minimax vs Rainbow\n",
      "Agent A won 10 times\n",
      "Agent B won 0 times\n",
      "Draw 0 times\n",
      "([1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [-1, -1, -1, -1, -1, -1, -1, -1, -1, -1])\n"
     ]
    }
   ],
   "source": [
    "# Facing different agents\n",
    "print(\"Facing MCTS vs Minimax\")\n",
    "res = facing_agents(mcts_agent, minimax_agent)\n",
    "\n",
    "print(\"\\nFacing MCTS vs Rainbow\")\n",
    "res = facing_agents(mcts_agent, rainbow_agent)\n",
    "\n",
    "print(\"\\nFacing MCTS vs PPO\")\n",
    "res = facing_agents(minimax_agent, ppo_agent)\n",
    "\n",
    "print(\"\\nFacing Minimax vs Rainbow\")\n",
    "res = facing_agents(minimax_agent, rainbow_agent)\n",
    "\n",
    "print(\"\\nFacing Minimax vs PPO\")\n",
    "res = facing_agents(minimax_agent, ppo_agent)\n",
    "\n",
    "print(\"\\nFacing Rainbow vs PPO\")\n",
    "res = facing_agents(rainbow_agent, ppo_agent)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ConnectX",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
